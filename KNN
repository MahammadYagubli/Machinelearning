#import all libraries which we can need
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
#famous iris dataset which is 3 types of the flowers that 
from sklearn.datasets import load_iris
iris =load_iris()
iris.feature_names
#here we convert array to the dataFrame and adding data column names as a title of the dataset 
df=pd.DataFrame( iris.data, columns=iris.feature_names)
df['label']=iris.target
#we create  scatter to be able to see the dots and visualize
sns.scatterplot(data=df, x='petal length (cm)', y='petal width (cm)', hue='label', palette='coolwarm')
#to use module we import KNeigghborClassifier module
from sklearn.neighbors import KNeighborsClassifier
#we already talked about n_neighbor it mean how many dots we need , 
#which method we need to use to calculate kneighbors, p values also mentioned before
knn=KNeighborsClassifier(n_neighbors=6, metric='minkowski', p=1);
#here we are adding x and y value y is result x is the values to result y
x=df.drop(['label'], axis=1)
y=df['label']

df['label']
#here we fit the x and y to have values
knn.fit(x,y)
#we are creating new value to be able to predict the result of the our result for the test 
x_n=np.array([[5.6, 3.4,1.4,0.1]])
x_n
knn.predict(x_n)
x_n2=np.array([[7.5, 4,5.5,2]])
knn.predict(x_n2)
#here we need to use to have some part of the our data to test our model evaluation
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split( x, y, test_size=0.2, train_size=0.8,
                                                    random_state=88, shuffle=True, stratify=y)
from sklearn.neighbors import KNeighborsClassifier
#we already talked about n_neighbor it mean how many dots we need , 
#which method we need to use to calculate kneighbors, p values also mentioned before
knn=KNeighborsClassifier(n_neighbors=6, metric='minkowski', p=1);
#rather than giving the valuie that we used before here we just only use test data some part of the our given data
knn.fit(X_train, y_train)
predicted_type=knn.predict(X_test)
#here we are going to use our accuracy_score method to find how much accurate is pur data
from sklearn.metrics import accuracy_score
accuracy_score(y_test, predicted_type)
#here how can we make our model to look much more better an daccurate
from sklearn.metrics import classification_report, confusion_matrix
#shows how accurate is our test
print(confusion_matrix(y_test,predicted_type ))
print(classification_report(y_test,predicted_type ))
#we ceate error rate and to check how does our error increase when we will be increasing knn neighbors value 
error_rate =[]
for i in range(1,40):
    knn=KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    pred_i=knn.predict(X_test)
    error_rate.append(np.mean(pred_i!=y_test))
    
    plt.figure(figsize=(10,6))
plt.plot(range(1,40),error_rate, color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)
plt.title('Error Rate vs K value')
plt.xlabel('k')
plt.ylabel('Error rate')
